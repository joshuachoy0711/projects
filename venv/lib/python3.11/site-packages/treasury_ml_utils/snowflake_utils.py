from typing import Optional, Union
import os

import pandas as pd
from snowflake.connector.pandas_tools import pd_writer
from snowflake.sqlalchemy import URL
from sqlalchemy import create_engine
from sqlalchemy.engine import Engine

ANALYTIC_DB_CONN_DETAILS = {
    'account': 'rt27428.eu-central-1',
    'database': 'ANALYTICS_DB',
    'warehouse': 'ANALYSTS',
    'authenticator': 'externalbrowser',
}


# Make the wrapper object a Borg for maximal reuse of the connection
class Borg:
    __shared_state: dict = {}

    def __init__(self):
        self.__dict__ = self.__shared_state
        self.engine = None


class SnowflakeConnector(Borg):

    def __init__(
            self,
            email: str):
        super().__init__()
        self.email = email
        self.engine: Optional[Engine] = self._get_engine()

    def _get_engine(self) -> Engine:

        engine = create_engine(
            URL(
                user=self.email,
                **ANALYTIC_DB_CONN_DETAILS
            )
        )

        try:
            connection = engine.connect()
            results = connection.execute('select current_version()').fetchone()
            print(results[0])
        except Exception as exception:
            print(exception)
            raise ValueError('Check credentials and/or IP address of connection.') from exception

        return engine

    def fetch(self, query: str, lower_case=True) -> pd.DataFrame:

        assert self.engine, 'No engine found.'

        conn = self.engine.connect()
        data = pd.read_sql(
            sql=query,
            con=conn
        )
        if lower_case:
            data.columns = [x.lower() for x in data.columns]

        conn.close()

        return data

    def upload_df(
            self,
            data_df: pd.DataFrame,
            table_name: str,
            schema: str = None,
            database: str = None,
            if_exists: str = 'append') -> None:
        """
        Upload a pandas dataframe to Snowflake.

        Arguments:
            data_df: dataframe to upload.
            table_name: destination table name in snowflake.
            schema: destination schema in snowflake.
            database: destination database in snowflake.
            if_exists: handle mode when table already exist,
            possible mode ("append", "replace", "fail")
        """
        print('Start uploading data to Snowflake.')
        conn = self.engine.connect()  # type: ignore

        if database:
            self.engine.execute(f'USE DATABASE {database};')  # type: ignore
        if schema:
            self.engine.execute(f'USE SCHEMA {schema};')  # type: ignore

        data_df.columns = data_df.columns.str.upper()
        data_df.to_sql(
            name=table_name,
            con=self.engine,
            index=False,
            if_exists=if_exists,
            method=pd_writer
        )

        conn.close()
        print('Finished uploading data to Snowflake.')

    def merge_df(
            self,
            data_df: pd.DataFrame,
            table_name: str,
            cols_keys: Union[str, list],
            schema: str = None,
            database: str = None) -> None:
        """
        Merge into table using a pandas dataframe to Snowflake.

        Arguments:
            data_df: dataframe to upload.
            table_name: destination table name in snowflake.
            cols_keys: column name(s) of key to merge on.
            schema: destination schema in snowflake.
            database: destination database in snowflake.
        """
        print('Start merging data into Snowflake.')
        conn = self.engine.connect()  # type: ignore

        if database:
            conn.execute(f'USE DATABASE {database};')  # type: ignore
        if schema:
            conn.execute(f'USE SCHEMA {schema};')  # type: ignore

        if isinstance(cols_keys, str):
            cols_keys = [cols_keys]
        stage_name = table_name
        cols_all = list(data_df.columns)
        cols_values = [col for col in cols_all if col not in cols_keys]

        create_stage_query = f'CREATE OR REPLACE STAGE {stage_name}_stage;'
        _ = conn.execute(create_stage_query)

        data_df.to_parquet(f'{table_name}.txt',
                           compression='snappy', index=False)

        put_query = f'PUT file:///{os.getcwd()}/{table_name}.txt @{stage_name}_stage;'
        _ = conn.execute(put_query)

        create_format_query = "create or replace file format parquet type ='PARQUET';"
        _ = conn.execute(create_format_query)

        select_cols = ', '.join([f'$1:{col} as {col}' for col in cols_all])
        match_clause = ' AND '.join([f't.{col} = s.{col}' for col in cols_keys])
        when_match_clause = ', '.join([f't.{col} = s.{col}' for col in cols_values])
        when_not_clause = ', '.join(cols_all)
        merge_query = f"""
        MERGE INTO {table_name} t USING(
        SELECT {select_cols}
        FROM @{stage_name}_stage (FILE_FORMAT =>parquet)) s
            ON {match_clause}
            WHEN MATCHED THEN
                UPDATE SET {when_match_clause}
            WHEN NOT MATCHED THEN
                INSERT ({when_not_clause}) VALUES ({when_not_clause});
        """
        _ = conn.execute(merge_query)

        conn.close()
        os.remove(f'{os.getcwd()}/{table_name}.txt')

        print('Finished merging data into Snowflake.')
