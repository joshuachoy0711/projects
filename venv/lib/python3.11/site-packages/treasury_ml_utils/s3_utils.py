from typing import Any, Optional, Tuple
import logging
from pathlib import Path
import pickle

import boto3
from botocore.exceptions import ClientError

from treasury_ml_utils.errors import FileCouldNotBeSavedToS3Exception, FileDoesNotExistInS3Exception


def decompose_s3_url(save_loc: str) -> Tuple[str, str]:
    save_loc = save_loc[5:]
    bucket = save_loc.split('/')[0]
    s3_file_name = save_loc[(len(bucket) + 1):]
    return bucket, s3_file_name


def file_exists_in_s3(s3_url: str) -> bool:
    bucket, s3_file_name = decompose_s3_url(s3_url)
    s3_resource = boto3.resource('s3')

    bucket_con = s3_resource.Bucket(bucket)
    obj = list(bucket_con.objects.filter(Prefix=s3_file_name))
    return len(obj) > 0


def save_file_to_s3(
        local_file_name: str,
        s3_url: str,
        profile_name: Optional[str] = None) -> None:
    bucket, s3_file_name = decompose_s3_url(s3_url)
    if profile_name:
        s3_client = boto3.Session(profile_name=profile_name).client('s3')
    else:
        s3_client = boto3.client('s3')
    try:
        s3_client.upload_file(local_file_name, bucket, s3_file_name)
    except ClientError as exception:
        logging.error(exception)
        raise FileCouldNotBeSavedToS3Exception from exception


def get_file_from_s3(
        s3_url: str,
        cache_loc: Optional[str] = None,
        profile_name: Optional[str] = None) -> Any:
    """
    Returns a pd.DataFrame of a file in S3 given the S3 url
    :param
        s3_url: S3 file path.
        cache_loc: Optional parameter for the location to save the pickled file at.
    :return: Any unpickled object
    """
    bucket, s3_file_name = decompose_s3_url(s3_url)
    file_name = s3_file_name.split('/')[-1]

    if cache_loc is not None:
        file = Path(cache_loc + f'/{file_name}')
        if file.is_file():
            with open(file, 'rb') as stream:
                return pickle.load(stream)

    if profile_name:
        client = boto3.Session(profile_name=profile_name).client('s3')
    else:
        client = boto3.client('s3')

    try:
        obj = client.get_object(Bucket=bucket, Key=s3_file_name)['Body'].read()
        if cache_loc is not None:
            with open(Path(cache_loc + f'/{file_name}'), 'wb') as stream:
                pickle.dump(obj, stream)
        return obj
    except ClientError as exception:
        if exception.response['Error']['Code'] == '404':
            raise FileDoesNotExistInS3Exception from exception
        raise ClientError from exception


def clean_directory_s3(s3_url: str):
    """
    Clean the directory in S3. Mainly used together with COPY INTO command.
    :param s3_url: S3 directory path
    """
    bucket, s3_file_name = decompose_s3_url(s3_url)
    client = boto3.client('s3')

    paginator = client.get_paginator('list_objects_v2')

    # check directory is not empty
    try:
        client.list_objects_v2(Bucket=bucket, Prefix=s3_file_name)
    except KeyError:
        logging.warning(f'Nothing in directory: {s3_url}')
        return

    # gather files to delete
    files_to_delete = []
    for page in paginator.paginate(Bucket=bucket, Prefix=s3_file_name):
        for file in page.get('Contents', []):
            files_to_delete.append({'Key': file['Key']})

    # delete objects in batches of 1000 - delete_objects API has a limit of 1000 objects per request
    for i in range(0, len(files_to_delete), 1000):
        batch = files_to_delete[i:i + 1000]
        client.delete_objects(Bucket=bucket, Delete={'Objects': batch})

    # check if directory is cleaned properly
    check_response = client.list_objects_v2(Bucket=bucket, Prefix=s3_file_name)
    if 'Contents' in check_response:
        raise Exception('Directory not cleaned properly.')
